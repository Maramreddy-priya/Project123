{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e4494e-0700-4890-867b-eac78a91321a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|    month|Total profit|\n+---------+------------+\n|     July|       -2138|\n| November|       10253|\n| February|        8465|\n|  January|        9684|\n|    March|        7793|\n|  October|        2959|\n|      May|       -3730|\n|   August|        2068|\n|    April|        4192|\n|     June|         420|\n| December|       -1604|\n|September|       -1399|\n+---------+------------+\n\n+-----------------+------------+\n|            State|Total Amount|\n+-----------------+------------+\n|         Nagaland|       11993|\n|        Karnataka|       12520|\n|       Tamil Nadu|        6276|\n|   Andhra Pradesh|       13256|\n|   Madhya Pradesh|       87463|\n|           Punjab|       16786|\n|              Goa|        6705|\n| Himachal Pradesh|        8666|\n|Jammu and Kashmir|       10829|\n|          Haryana|        8863|\n|          Gujarat|       21371|\n|           Sikkim|        5276|\n|            Delhi|       22957|\n|        Rajasthan|       22334|\n|          Kerala |       13871|\n|      Maharashtra|      102498|\n|      West Bengal|       14328|\n|            Bihar|       13417|\n|    Uttar Pradesh|       38362|\n+-----------------+------------+\n\n+-------------------+\n|insert_timestamp   |\n+-------------------+\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n|2024-08-20 22:32:51|\n+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType, TimestampType, FloatType\n",
    "from pyspark.sql.functions import current_timestamp, sum, quarter, month, date_format, current_date\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retails sales\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "dbutils.secrets.listScopes()\n",
    "dbutils.secrets.list('secretscope')\n",
    "service_credential = dbutils.secrets.get(scope=\"secretscope\", key=\"secretid\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.salesprojectstorage.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.salesprojectstorage.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.salesprojectstorage.dfs.core.windows.net\", \"cfdf2f4b-0dd6-4003-85c6-42f61c6e1fe7\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.salesprojectstorage.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.salesprojectstorage.dfs.core.windows.net\", \"https://login.microsoftonline.com/80d8b5a9-a2e9-4997-a38b-d0e9c44ef5d6/oauth2/token\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('path', StringType()),\n",
    "    StructField('name', StringType()),\n",
    "    StructField('size', IntegerType()),\n",
    "    StructField('modificationTime', LongType())\n",
    "])\n",
    "\n",
    "directory_path = \"abfs://silver@salesprojectstorage.dfs.core.windows.net\"\n",
    "file_list = dbutils.fs.ls(directory_path)\n",
    "\n",
    "df = spark.createDataFrame(data=file_list, schema=schema)\n",
    "\n",
    "df = df.select('path', 'name', 'size', (df.modificationTime/1000).cast('timestamp').alias('UpdatedTime'))\n",
    "df_latestfile = df.select('path', 'name', 'size', 'UpdatedTime').orderBy(df.UpdatedTime.desc()).limit(1)\n",
    "\n",
    "# Extract the file path from df_latestfile\n",
    "file_path = df_latestfile.select('path').collect()[0][0]\n",
    "\n",
    "Schema = StructType([\n",
    "    StructField(\"Order_ID\", StringType()),\n",
    "    StructField(\"Order_Date\", DateType()),\n",
    "    StructField(\"CustomerName\", StringType()),\n",
    "    StructField(\"State\", StringType()),\n",
    "    StructField(\"City\", StringType()),\n",
    "    StructField(\"Order ID\", StringType()),\n",
    "    StructField(\"Amount\", IntegerType()),\n",
    "    StructField(\"Profit\", IntegerType()),\n",
    "    StructField(\"Quantity\", IntegerType()),\n",
    "    StructField(\"Category\", StringType()),\n",
    "    StructField(\"Sub-Category\", StringType()),\n",
    "    StructField(\"PaymentMode\", StringType()),\n",
    "])\n",
    "\n",
    "df_readingfile = spark.read.csv(file_path, header=True, schema=Schema)\n",
    "df_readingfile = df_readingfile.drop(\"Order ID\")\n",
    "df_readingfile = df_readingfile.withColumn(\"Timestamp\", current_timestamp()) \\\n",
    "                               .withColumn(\"quarter\", quarter(df_readingfile['Order_Date']))\\\n",
    "                               .withColumn(\"insert_timestamp\", date_format(col(\"Timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "df_profit_Bymonth = df_readingfile.groupBy(date_format(df_readingfile['Order_Date'],\"MMMM\").alias(\"month\")).agg(sum(\"Profit\").alias(\"Total profit\"))\n",
    "df_profit_Bymonth.show()\n",
    "\n",
    "df_totalamount_Bystate = df_readingfile.groupBy(\"State\").agg(sum(\"Amount\").alias(\"Total Amount\"))\n",
    "\n",
    "df_totalamount_Bystate.show()\n",
    "\n",
    "df_totalquantity_paymentmode = df_readingfile.groupBy(\"PaymentMode\").agg(sum(\"Quantity\").alias(\"Total Quantity\"))\n",
    "\n",
    "\n",
    "\n",
    "df_readingfile.write.mode(\"append\").parquet(\"abfs://gold@salesprojectstorage.dfs.core.windows.net/transformedfile\")\n",
    "df_profit_Bymonth.write.mode(\"append\").parquet(\"abfs://gold@salesprojectstorage.dfs.core.windows.net/profitBymonthtransformedfile\")\n",
    "df_totalamount_Bystate.write.mode(\"append\").parquet(\"abfs://gold@salesprojectstorage.dfs.core.windows.net/totalAmountByStatetransformedfile\")\n",
    "df_totalquantity_paymentmode.write.mode(\"append\").parquet(\"abfs://gold@salesprojectstorage.dfs.core.windows.net/totalquantityBypaymentmodetransformedfile\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
